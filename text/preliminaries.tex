%---------------------------------------------------------------
\chapter{Preliminaries}
%---------------------------------------------------------------

Before proceeding to the main parts of the work,
let us begin with a couple of definitions and theory the reader might find useful to fully understand later chapters.
This chapter introduces preliminaries on graph theory, including graph problems and centrality measures, and 
classical computational complexity followed by parameterized complexity.


%---------------------------------------------------------------
\section{Graph theory}

This section introduces basic concepts from graph theory.
For a thorough revision on this topic, please refer to the monograph of Diestel \cite{Diestel2018}.

% Graph
\begin{definition}[Simple graph]
    A \emph{simple graph} is an ordered pair $G=(V,E)$, where $V$ and $E$ are disjoint sets,
    $V$ is a finite nonempty set of arbitrary items called vertices (or nodes) and 
    $E$ is a finite set of unordered pairs of vertices called edges, $E \subseteq \{\{x,y\} \mid x,y \in V \wedge x \neq y\}$.
\end{definition}

Each graph used in this thesis is simple graph, for this reason,
we omit the word ``simple'' through the text and call each graph just ``graph''.
Also, the terms ``graph'' and ``network'' are interchangeable in this thesis as we only use graphs to describe networks.
By network we mean such graph, where vertices represents members of the network 
and edges represents, in some sense, a connection between two given members, e.g., enabling their communication.

Sets of all vertices $V$ and edges $E$ of graph $G$ are typically denoted as $V(G)$ and $E(G)$ respectively.
A number of vertices in graphs $G$ is typically denoted as $n$, $|V(G)| = n$ and
a number of edges in $G$ is typically denoted as $m$, $|E(G)| = m$.

% Degree
\begin{definition}[Vertex degree]
    A degree of vertex $v$ in some graph $G$, denoted as $\deg(v)$, is the number of neighbors of $v$.
    In other words, it is the number of edges vertex $v$ is a part of, i.e.,
    $\Big|\{x \in V(G) \vert \{v,x\} \in E(G) \vee \{x,v\} \in E(G)\}\Big|$.
\end{definition}

A set of neighbors of some vertex $v$ is often denoted as $N(v)$, so then $\deg(v) = |N(v)|$.

% r-regular graph
\begin{definition}[$r$-regular graph]
    A graph $G$ is \emph{$r$-regular} if the degree of each vertex from $V(G)$ is $r$, i.e., $\forall v \in V(G) \colon \deg(v) = r$.
\end{definition}

% Regular graph
\begin{definition}[Regular graph]
    A graph $G$ is \emph{regular} if there exists some $r \in \mathbb{N}$ for which~$G$ is $r$-regular.
\end{definition}

% Complement graph
\begin{definition}[Complement graph]
    A \emph{complement graph} $\overline{G}$ of graph $G$ is the graph on vertices $V(G)$,
    where two vertices are adjacent (there is an edge between them) if and only if
    they are not adjacent in $G$, i.e., $\forall e \in V(G) \times V(G) \colon e \in E(\overline{G}) \Leftrightarrow e \notin E(G)$.
\end{definition}

% Induced subgraph
\begin{definition}[Induced subgraph]
    An \emph{induced subgraph} $G[S]$ of graph $G$ is the graph on vertices $V(G[S]) = S \subset V(G)$,
    where two vertices are adjacent if and only if they are adjacent in $G$, i.e.,
    $(\forall u,v \in S)(\{u,v\} \in E(G[S]) \Leftrightarrow \{u,v\} \in E(G))$.
\end{definition}

% Complete graph
\begin{definition}[Complete graph]
    A \emph{complete graph} is the graph $G$ if there is an edge between every pair of vertices from $V(G)$, i.e.,
    $(\forall u,v \in V(G))(\{u,v\} \in E(G))$.
\end{definition}

% Clique
\begin{definition}[Clique]
    A clique $C$ is the subgraph of graph $G$ where $G[E(C)]$ is a complete graph.
\end{definition}

% Vertex cover
\begin{definition}[Vertex cover]
    A \emph{vertex cover} of graph $G$ is the subset $S \subseteq V(G)$ where each edge from $E(G)$ is
    covered by some vertex from $S$. In other words, at least one endpoint of each edge is present in $S$, i.e.,
    $(\forall \{u,v\} \in E(G))(u \in S \vee v \in S)$.
\end{definition}


\subsection{Graph problems}

What follows is a definition of the \NPh graph problem we will use in Proof \ref{proofDB}.

% k-Clique
\begin{definition}[$k$-\textsc{Clique} problem]
    Given graph $G$, the $k$-\textsc{Clique} problem is to determine if
    there exists a clique $C$ in $G$ where $|V(C)| = k$.
\end{definition}

There are many other graph problems related to cliques. The $k$-\textsc{Clique} problem is a decision problem but
there are also optimization variants, for example the \textsc{Maximum Clique} problem.


\subsection{Centrality measures}

Centrality measure, or centrality for short, is a measure from graph theory which is widely used in social network analysis.
Generally, centrality is a function $c: G \times V \rightarrow \mathbb{R}$ which describes the importance of a node in given network.
With centrality, we can measure a ranking (position among other nodes) of given node within their network.
What follows is a definition of a degree centrality introduced by Shaw \cite{Shaw1954}, but there are other centralities used in SNA like
closeness~\cite{Beauchamp1965}, betweenness \cite{Anthonisse1971,Freeman1977} or core \cite{Seidman1983} centrality.

\begin{definition}[Degree centrality]
    A degree centrality measures an importance of a vertex by its degree.
    A degree centrality of the vertex $v$ in network $G$ is defined as:
    $$c_{deg}(G, v) = \deg(v).$$
\end{definition}


%---------------------------------------------------------------
\section{Classical computational complexity}

In this section, we briefly describe the two most fundamental classical complexity classes, \Po and \NP.
Detailed revision on this topic can be found in the textbook from Arora and Barak \cite{Arora2009}.

Let us start with defining the \textsf{DTIME} complexity class, which we then use to define the \Po class.

\begin{definition}[\textsf{DTIME}]
    A complexity class \textsf{DTIME}(f(n)) is the set of all decision problems that are computable in time
    $c \cdot f(n)$ for some constant $c > 0$ and some function $f \colon \mathbb{N} \rightarrow \mathbb{N}$.
\end{definition}

% P
\begin{definition}[\Po]
    A complexity class \Po is the set of all decision problems that are in class \textsf{DTIME}($n^c$)
    for any $c \geq 1$, i.e.
    $\cup_{c \geq 1} \textsf{DTIME}(n^c)$.
\end{definition}

Informally, complexity class \Po consists of all decision problems that can be solved in polynomial time.
Problems for which exists in practice efficient enough algorithm are called \emph{tractable}.

% NP
\begin{definition}[\NP]
    A complexity class \NP is the set of all decision problems $L$ for which there exist
    a polynomial $p \colon \mathbb{N} \rightarrow \mathbb{N}$ and a deterministic Turing machine $M$
    such that for every instance $x$ of $L$ $x$ is a $yes$-instance if and only if there exists
    a polynomial-size solution $u$ of $x$ that can be verified on $M$ in polynomial time.
    Such solution $u$ is then called the \emph{certificate} or the \emph{witness}.
\end{definition}

Informally, complexity class \NP consists of all decision problems that can be solved in polynomial time
when multiple steps can be done in parallel.
Note that $\Po \subseteq \NP$, because, for any problem from \Po, it takes only a polynomial amount of time to solve
it in the first place.

At the end of this section, let us briefly recall computational complexity terms \emph{hardness} and \emph{completeness}.
A problem $p$ is, given some complexity class \textsf{C}, \textsf{C-hard} if for any problem $c$ from~\textsf{C}
there exists a polynomial reduction from $c$ to $p$.
A problem $p$ is then \textsf{C-complete} if it is \textsf{C-hard} and it belongs to \textsf{C} at the same time.


%---------------------------------------------------------------
\section{Parameterized computational complexity}\label{section:ParamComp}

We use the textbook by Cygan et al. \cite{Cygan2015} as our main source of information on this topic since it
provides arguably the most complex and comprehensive overview of the theory of parameterized complexity and,
in many cases, presents the state of the art in the field.

The classical complexity analysis is often insufficient when dealing with \NPh problems.
The main reason is that there are \NPh problems that are in some sense harder than the others.
This inability of the classical approach to distinguish between the hardness of various \NPh problems
led to a development of the parameterized complexity theory.
Parameterized complexity is a direct generalization of the classical complexity theory and it arms its users
with the ability to analyze running time of algorithms, and thus computational complexities of problems
they try to solve, in a finer detail.
It gives us tools which help us to distinguish between different difficulty levels in places where
\NP-hardness fails to do so.
To achieve this, parameter complexity introduces a notion of parameterization of the input instance, where
the parameter is just some secondary measurement of the input instance.
Such parameter is then used together with the input instance size when describing a computational complexity of \NPh problems.
Given a problem, there is typically many parameters we can parameterize the problem by.
This leads us to the importance of choosing the right parameter.

When parameterizing some problem, we typically look for parameters which are small in real-world applications,
as one of the main goal when studying parameterized problems is to find algorithms for these problems in which
the running time is exponential only in the parameter, leaving the running time polynomial in the, potentially enormous, input size.
However, not every choice of the parameter let us design such an efficient algorithm, as it seems that many problems with
certain parameters admit no efficient algorithm at all.
That means the hardness, or tractability, of the problem depends on the parameter we use.
Typically, the more information a parameter carries about the input instance, the higher are chances for us to exploit this
information and design a faster algorithm.

The typical parameter we often try is the size, or other property, of the solution we are looking for.
The other type of parameter is a measure of some property of the input instance.
For example the maximum degree, regularity or treewidth measurings of the input graph.
For non-graph instances, it may be the maximum length of a string when the input instance consists of a set of strings,
or a number of variables when working with Boolean formulas.

Given problem parameterized by $p$, algorithm designers commonly talk about some function of $p$, $f(p)$.
Let us note that it is also possible to use more than one parameter.
Having parameters $k$ and $l$, we then talk about a function of $k$ and $l$, $f(k,l)$.
However, we can (and we actually do) express the parameterization by $k$ and $l$ by using just one parameter $k+l$, $f(k+l)$.

The two most fundamental complexity classes of this theory, used to reason about the complexity of parameterized problems,
are \FPT and \XP.
To distinguish between \NPh problems that are in \XP but not in \FPT, we can use another set of complexity classes, the \W-hierarchy.
There is also a complexity class called \pNP.
What follows are the formal definitions of some of these classes, together with definitions of parameterized problem
and parameterized reduction.
The hierarchy of the classes can be seen in Figure \ref{fig:complexityClasses}.


% Figure - complexity classes
\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \pgftransformscale{.8}
        %% Horizontal bar
        \draw[very thick] (6,0) -- (-6,0);
        % FPT
        \draw (-2,0) parabola bend (0,2) (2,0);
        \node at (0,1) {\FPT};
        % W[1]
        \draw (-2.5,0) parabola bend (0,3) (2.5,0);
        \node at (0,2.5) {\textsf{W}[1]};
        % W[2]
        \draw (-3,0) parabola bend (0,4) (3,0);
        \node at (0,3.5) {\textsf{W}[2]};
        % % W[t]
        \draw (-4,0) parabola bend (0,6) (4,0);
        \node[anchor=north] at (0,5.8) {
            \begin{tabular}{c}
                \textsf{W}[$t$] \\
                $\vdots$ \\
            \end{tabular}
        };
        % XP
        \draw[rounded corners] (-6,0) .. controls (-1,8) and (9,17) .. (6,0);
        \node at (4,8) {\XP};
        % para-NP
        \draw[rounded corners] (6,0) .. controls (1,8) and (-9,17) .. (-6,0);
        \node at (-4,8) {\pNP};
    \end{tikzpicture}
    \caption
    [Relations among the parameterized complexity classes]
    {
        Relations among the parameterized complexity classes.
        Inspired by the depiction from Flum and Grohe \cite[p.~97]{Flum2006}.
    }
    \label{fig:complexityClasses}
\end{figure}


% Parameterized problem
\begin{definition}[Parameterized problem]
    A parameterized problem is a language $L \subseteq \Sigma^* \times \mathbb{N}$, where
    $\Sigma$ is a fixed, finite alphabet and $\Sigma^*$ is a set of all strings over $\Sigma$.
    For an instance $(x, k) \in \Sigma^* \times \mathbb{N}$, $k$ is called the parameter.
\end{definition}

One particular problem parameterized by different parameters leads to different parameterized problems.

% Parameterized reduction
\begin{definition}[Parameterized reduction]
    Let $A,B \subseteq \Sigma^* \times \mathbb{N}$ be two parameterized problems.
    A parameterized reduction from $A$ to $B$ is an algorithm that, given an instance $(x, k)$ of $A$,
    outputs an instance $(x', k')$ of $B$ such that
    \begin{itemize}
        \item $(x, k)$ is a $yes$-instance of $A$ if and only if $(x', k')$ is a $yes$-instance of $B$,
        \item $k' \leq g(k)$ for some computable, nondecreasing function $g$, and
        \item the running time is $f(k) \cdot |x|^{\mathcal{O}(1)}$ for some computable, nondecreasing function $f$.
    \end{itemize}
\end{definition}

% FPT problem and class
\begin{definition}[FPT]
    A parameterized problem $L$ is called fixed-parameter tractable (FPT) if there exists
    \begin{itemize}
        \item an algorithm $\mathcal{A}$, called fixed-parameter algorithm, or FPT algorithm,
        \item a computable, nondecreasing function $f : \mathbb{N} \times \mathbb{N}$
        \item and a constant $c$
    \end{itemize}
    such that, given $(x,k) \in \Sigma^* \times \mathbb{N}$,
    the algorithm $\mathcal{A}$ correctly decides whether $(x, k) \in L$ in time bounded by
    $f(k) \cdot |(x,k)|^c$.
\end{definition}

The complexity class containing all fixed-parameter tractable problems is called \FPT.
An example of a problem from \FPT is $n$-variable \textsc{SAT} parameterized by $n$.
Indeed, the problem can be solved in time $\Theta(2^n)$ by simply trying each of the $2^n$
possible evaluations for the variables and checking if the evaluation is correct in time $\Theta(n)$.
Moreover, we know from the Exponential Time Hypothesis, formulated by Impagliazzo and Paturi \cite{Impagliazzo1999},
that the problem cannot be solved in time $2^{o(n)}$

The typical goal when designing FPT algorithms is to make factor $f(k)$ and constant exponent~$c$
in the running time boundary as small as possible.
We can see that if the parameter is equal to the input size, then \FPT become exponential and,
on the other hand if $k = 1$, then \FPT become~\Po~\cite{Koutensky2020}.

The only difference between FPT and para-NP problems is that algorithms for para-NP problems are
nondeterministic in general.
We do not provide the exact definition of para-NP problems as they are not important to us.
From the \pNP-hardness perspective, we can say that a problem is \pNPh if it is \NPh already for a constant value of the parameter.

% XP
\begin{definition}[XP]
    A parameterized problem $L$ is called slice-wise polynomial (XP) if there exists
    \begin{itemize}
        \item an algorithm $\mathcal{A}$
        \item and two computable, nondecreasing functions $f,g : \mathbb{N} \times \mathbb{N}$
    \end{itemize}
    such that, given $(x,k) \in \Sigma^* \times \mathbb{N}$,
    the algorithm $\mathcal{A}$ correctly decides whether $(x, k) \in L$ in time bounded by
    $f(k) \cdot |(x,k)|^{g(k)}$.
\end{definition}

The complexity class containing all slice-wise polynomial problems is called \XP.
An example of a problem from \XP is $k$-\textsc{Independent Set}.

Note that definitions of parameterized problems FTP and XP differs only in a running time boundary of algorithm $\mathcal{A}$
and so $\FPT \subseteq \XP$.
Although both FPT and XP algorithms run in polynomial time for every fixed value of the parameter,
the underlying difference between them is that FPT algorithms have the combinatorial explosion
restricticted to the parameter only, leaving the exponent of the instance size constant, in other words,
FPT algorithms are more efficient than XP algorithms.


\subsection{\W-hierarchy}

As stated by Cygan et al. \cite[p.~423]{Cygan2015},
there are thousands of natural problems which are \NPc and which are reducible to each other,
meaning that in this sense, they are equally hard and thus we can say that
they occupy the same level of hardness.
However, we cannot say the same when talking about parameterized problems as it seems there are
different levels of hardness for such problems and, in this sense, even basic problems
seem to be differently hard as they occupy different hardness levels.
For this reason, \W-hierarchy was introduced by Downey and Fellows \cite{Downey1999} as
an attempt to shed a light on these apparent differences in hardness of parameterized problems. 

The levels of \W-hierarchy are marked as \textsf{W}[$t$] for $t \in \mathbb{N} \wedge t \geq 0$,
where each level represents its own complexity class.
The most important level for us will be \Wone,
as well as the fact that the $k$-\textsc{Clique} problem is \Wone-complete \cite{Downey1999}.

We do not provide the exact definition and further description of \W-hierarchy
because it is not important for the purposes of this work.
On the other hand, what is important is a fact that we interpret the \W-hardness as an evidence that
a problem is not fixed-parameter tractable.
This interpretation is based on a general assumption that $\FPT \neq \Wone$.
To show that certain problem is \Wh, it is typically shown a parameterized reduction from some other problem,
which is already known to be \Wh, to it.
