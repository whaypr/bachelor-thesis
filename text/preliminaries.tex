%---------------------------------------------------------------
\chapter{Preliminaries}
%---------------------------------------------------------------

Before proceeding to the main parts of the work,
let us begin with a couple of definitions and theory the reader might find useful to fully understand later chapters.
This chapter introduces preliminaries on graph theory, including graph problems and centrality measures, and 
classical computational complexity followed by parameterized complexity.


%---------------------------------------------------------------
\section{Graph theory}

This section introduces basic concepts from graph theory.
For a thorough revision on this topic, please refer the graph theory textbook from Diestel \cite{Diestel2018}.

% Graph
\begin{definition}[Graph]
    Graph $G$ is an ordered pair of disjoint sets $V$ and $E$, $G=(V,E)$.
    Set $V$ is a finite set of arbitrary items called vertices (or nodes).
    Set $E$ is a finite set of unordered pairs of vertices called edges, $E \subseteq \{\{x,y\} \mid x,y \in V \wedge x \neq y\}$.
    Sets of all vertices $V$ and edges $E$ of graph $G$ are in general denoted by $V(G)$ and $E(G)$ respectively.
\end{definition}
We use a standard graph definition, where graphs are undirected (edges are not ordered),
unweighted (edges has uniformed ``value''), without loops (edges are pairs of distinct vertices) and
without multiple edges (edges are unique pairs).
These graphs are called \emph{simple}.

In this thesis, the terms ``graph'' and ``network'' are interchangeable as we only use graphs to describe networks.
By network we mean such graph, where vertices represents members of the network 
and edges represents a, in some sense, connection between two given members, e.g., enabling their communication.

% Degree
\begin{definition}[Vertex degree]
    Degree of vertex $v$ in some graph $G$, denoted as deg(v), is the number of neighbors of v.
    In other words, it is the number of edges vertex $v$ is a part of, i.e.,
    $\Big|\{x \in V(G) \vert \{v,x\} \in E(G) \vee \{x,v\} \in E(G)\}\Big|$.
\end{definition}
The number of neighbors of some vertex $v$ can be also denoted as $N(v)$, thus $deg(v) = N(v)$.

% r-regular graph
\begin{definition}[$r$-regular graph]
    Graph $G$ is $r$-regular if degree of each vertex from $V(G)$ is $r$, i.e., $(\forall v \in V(G))(deg(v) = r)$.
\end{definition}

% Regular graph
\begin{definition}[Regular graph]
    Graph $G$ is regular if there exists some $r \in \mathbb{N}$ for which $G$ is $r$-regular.
\end{definition}

% Complement graph
\begin{definition}[Complement graph]
    Complement graph $\overline{G}$ of graph $G$ is the graph on vertices $V(G)$,
    where two vertices are adjacent (there is an edge between them) if and only if
    they are not adjacent in $G$, i.e., $(\forall e \in V(G) \times V(G))(e \in E(\overline{G}) \Leftrightarrow e \notin E(G))$.
\end{definition}

% Induced subgraph
\begin{definition}[Induced subgraph]
    Induced subgraph $G[S]$ of graph $G$ is the graph on vertices $V(G[S]) = S \subset V(G)$,
    where two vertices are adjacent if and only if they are adjacent in $G$, i.e.,
    $(\forall u,v \in S)(\{u,v\} \in E(G[S]) \Leftrightarrow \{u,v\} \in E(G))$.
\end{definition}

% Complete graph
\begin{definition}[Complete graph]
    Complete graph is the graph $G$ if there is an edge between every pair of vertices from $V(G)$, i.e.,
    $(\forall u,v \in V(G))(\{u,v\} \in E(G))$.
\end{definition}

% Clique
\begin{definition}[Clique]
    Clique $C$ is the subgraph of graph $G$ where $G[E(C)]$ is a complete graph.
\end{definition}

% Vertex cover
\begin{definition}[Vertex cover]
    Vertex cover of graph $G$ is the subset $S \subseteq V(G)$ where each edge from $E(G)$ is
    covered by some vertex from $S$. In other words, at least one endpoint of each edge is present in $S$, i.e.,
    $(\forall \{u,v\} \in E(G))(u \in S \vee v \in S)$.
\end{definition}


\subsection{Graph problems}

Here are definitions of some common \NPh graph problems we will need.

% k-Clique
\begin{definition}[$k$-\textsc{Clique} problem]
    Given graph $G$, the $k$-\textsc{Clique} problem is to determine if
    there exists a clique $C$ in $G$ where $|V(C)| = k$.
\end{definition}
There are many other graph problems related to cliques. The $k$-\textsc{Clique} problem is a decision problem but
there are also optimization variants, for example the \textsc{Maximum Clique} problem.

% Minimum Vertex Cover
\begin{definition}[\textsc{Minimum Vertex Cover} problem]
    Given graph $G$, the \textsc{Minimum Vertex Cover} problem is to find
    the vertex cover $M$ that there is no other vertex cover $N$ with fewer vertices than $M$, i.e., 
    $(\nexists N)(|N| < |M|)$.
\end{definition}
Do not confuse \emph{minimum vertex cover} from the \textsc{Minimum Vertex Cover} problem defined above with \emph{minimal vertex cover},
which is the vertex cover such that any of its non-trivial subsets is not a vertex cover.
Every \emph{minimum vertex cover} is also \emph{minimal vertex cover} but not necessarily vice versa.


\subsection{Centrality measures}

Centrality measure, or centrality for short, is a measure from graph theory which is widely used in social network analysis.
Generally, centrality is a function $c: G \times V \rightarrow \mathbb{R}$ which describes the importance of a node in given network.
With centrality, we can measure a ranking (position among other nodes) of given node within their network.
What follows is a definition of a degree centrality introduced by Shaw \cite{Shaw1954} but there are other centralities used in SNA like
closeness \cite{Beauchamp1965}, betweenness \cite{Anthonisse1971,Freeman1977} or core \cite{Seidman1983} centrality.

\begin{definition}[Degree centrality]
    A degree centrality measures an importance of a vertex by its degree.
    A degree centrality of the vertex $v$ in network $G$ is defined as:
    $$c_{deg}(G, v) = deg(v).$$
\end{definition}


%---------------------------------------------------------------
\section{Classical computational complexity}

In this section we describe the two most fundamental complexity classes.
Detailed revision on this topic --
including definitions on formal languages, Turing machines and their programs --
can be found in the textbook from Garey and Johnson \cite{Garey1990}. 

% P
\begin{definition}[\Po]
    Complexity class \Po is a set of all languages $L$ for which there is a polynomial time DTM program $M$
    such that $L$ is recognized by $M$.
\end{definition}
Informally, complexity class \Po consists of all decision problems that can be solved in polynomial time.
Problems for which exists in practice efficient enough algorithm are called \emph{tractable}.
These problems are typically in \Po.

% NP
\begin{definition}[\NP]
    Complexity class \NP is a set of all languages $L$ for which there is a polynomial time NDTM program $M$
    such that $L$ is recognized by $M$.
\end{definition}
Informally, complexity class \NP consists of all decision problems that can be solved in polynomial time
when multiple steps can be done in parallel.
Alternatively, we can say that in \NP are such problem, that their solution can be verified in polynomial time.
Note that definitions of classes \Po and \NP differs only in a determinism/nondeterminism of the Turing machine.

Lastly, let us briefly recall computational complexity terms \emph{hardness} and \emph{completeness}.
A problem $p$ is, given some complexity class \textsf{C}, \textsf{C-hard} if for any problem $c$ from \textsf{C}
there exists a polynomial reduction from $c$ to $p$.
A problem $p$ is then \textsf{C-complete} if it is \textsf{C-hard} and it belongs to \textsf{C} at the same time.


%---------------------------------------------------------------
\section{Parameterized computational complexity}\label{section:ParamComp}

The foundations of parameterized complexity were laid by Downey and Fellows
in the series of papers from years 1992 to 1995 \cite{Downey1992,Downey1995.1,Downey1995.2,Downey1993,Downey1995.4},
which the authors further presented later in 1999 in their book \cite{Downey1999},
which was refined and once more published in 2013 \cite{Downey2013}.
Other relevant and potentially useful literature on this topic are
two books by Flum with Grohe \cite{Flum2006} and by Niedermeier \cite{Niedermeier2006} from 2006, book by Hans et al. \cite{Hans2012} from 2012,
book by Cygan et al. \cite{Cygan2015} from 2015 (which to a great extent covers knowledge from the previous literature)
and two books by Haan \cite{Haan2019} and by Fomin et al. (focused mainly on kernelization) \cite{Fomin2019} from 2019.
However, keep in mind that there is a highly active line of research in the field of parameterized complexity, so,
as researchers keep making new discoveries, information presented in the work of mentioned authors may not necessarily be up-to-date.
We use textbook by Cygan et al. \cite{Cygan2015} as our main source of information on this topic since it
provides arguably the most complex and comprehensive overview of the theory of parameterized complexity and,
in many cases, presents the state of the art in the field.

The classical complexity analysis is often insufficient when dealing with \NPh problems.
The main reason is that there are \NPh problems that are in some sense harder than the others.
This inability of the classical approach to distinguish between the hardness of various \NPh problems
led to a development of the parameterized complexity theory.
Parameterized complexity is a direct generalization of the classical complexity theory and it arms its users
with the ability to analyze running time of algorithms, and thus computational complexities of problems
they try to solve, in a finer detail.
It gives us tools which help us to distinguish between different difficulty levels in places where
\NP-hardness fails to do so.
To achieve this, parameter complexity introduces a notion of parameterization of the input instance, where
the parameter is just some secondary measurement of the input instance.
Such parameter is then used together with the input instance size when describing a computational complexity of \NPh problems.
Given a problem, there is typically many parameters we can parameterize the problem by.
This leads us to the importance of choosing the right parameter.

When parameterizing some problem, we typically look for parameters which are small in real-world application,
as one of the main goal when studying parameterized problems is to find algorithms for these problems in which
the running time is exponential only in the parameter, leaving the running time polynomial in the, potentially enormous, input size.
However, not every choice of the parameter let us design such an efficient algorithm, as it seems that many problems with
certain parameters admit no efficient algorithm at all.
That means the hardness, or tractability, of the problem depends on the parameter we use.
Typically, the more information a parameter carries about the input instance, the higher are chances for us to exploit this
information and design a faster algorithm.

The typical parameter we often try is the size, or other property, of the solution we are looking for.
Other type of parameter is a measure of some property of the input instance.
For example the maximum degree, regularity or treewidth measurings of the input graph.
For non-graph instances it may be the maximum length of a string when the input instance consists of a set of strings,
or a number of variables when working with Boolean formulas.

Given problem parameterized by $p$, algorithm designers commonly talk about some function of $p$, $f(p)$.
Let us note that it is also possible to use more than one parameter.
Having parameters $k$ and $l$, we then talk about a function of $k$ and $l$, $f(k,l)$.
However, we can (and we actually do) express the parameterization by $k$ and $l$ by using just one parameter $k+l$, $f(k+l)$.

The two most fundamental complexity classes of this theory, used to reason about the complexity of parameterized problems,
are \FPT and \XP.
To distinguish between \NPh problems that are in \XP but not in \FPT, we can use another set of complexity classes, the W-hierarchy.
There is also complexity class called \pNP.
What follows are the formal definitions of some of these classes, together with definition of parameterized problem.   

% Parameterized problem
\begin{definition}[Parameterized problem]
    A parameterized problem is a language $L \subseteq \Sigma^* \times \mathbb{N}$, where
    $\Sigma$ is a fixed, finite alphabet and $\Sigma^*$ is a set of all strings over $\Sigma$.
    For an instance $(x, k) \in \Sigma^* \times \mathbb{N}$, $k$ is called the parameter.
\end{definition}

% FPT problem and class
\begin{definition}[FPT]
    A parameterized problem $L$ is called fixed-parameter tractable (FPT) if there exists
    \begin{description}
        \item an algorithm $\mathcal{A}$, called fixed-parameter algorithm, or FPT algorithm,
        \item a computable, nondecreasing function $f : \mathbb{N} \times \mathbb{N}$
        \item and a constant $c$
    \end{description}
    such that, given $(x,k) \in \Sigma^* \times \mathbb{N}$,
    the algorithm $\mathcal{A}$ correctly decides whether $(x, k) \in L$ in time bounded by
    $f(k) \cdot |(x,k)|^c$.
\end{definition}
The complexity class containing all fixed-parameter tractable problems is called \FPT.

The typical goal when designing FPT algorithms is to make factor $f(k)$ and constant exponent $c$
in the running time boundary as small as possible.
We can see that if the parameter is equal to the input size, then \FPT become equal to \NP and,
on the other hand if $k = 1$, then \FPT become \P \cite{Koutensky2020}.

% XP
\begin{definition}[XP]
    A parameterized problem $L$ is called slice-wise polynomial (XP) if there exists
    \begin{description}
        \item an algorithm $\mathcal{A}$
        \item and two computable, nondecreasing functions $f,g : \mathbb{N} \times \mathbb{N}$
    \end{description}
    such that, given $(x,k) \in \Sigma^* \times \mathbb{N}$,
    the algorithm $\mathcal{A}$ correctly decides whether $(x, k) \in L$ in time bounded by
    $f(k) \cdot |(x,k)|^{g(k)}$.
\end{definition}
The complexity class containing all slice-wise polynomial problems is called \XP.

Note that definitions of parameterized problems FTP and XP differs only in a running time boundary of algorithm $\mathcal{A}$.
Although both FPT and XP algorithms run in polynomial time for every fixed value of the parameter,
the underlying difference between them is that FPT algorithms have the combinatorial explosion
restricticted to the parameter only, leaving the exponent of the instance size constant, in other words,
FPT algorithms are more efficient that XP algorithms.
From this we can see that each problem from \FPT is trivialy also in \XP, $\FPT \subset \XP$.

% para-NP
\begin{definition}[para-NP]
    A parameterized problem $L$ is called para-NP if there exists
    \begin{description}
        \item a nondeterministic algorithm $\mathcal{A}$,
        \item a computable, nondecreasing function $f : \mathbb{N} \times \mathbb{N}$
        \item and a constant $c$
    \end{description}
    such that, given $(x,k) \in \Sigma^* \times \mathbb{N}$,
    the algorithm $\mathcal{A}$ correctly decides whether $(x, k) \in L$ in time bounded by
    $f(k) \cdot |(x,k)|^c$.
\end{definition}
The complexity class containing all para-NP problems is called \pNP.

Note that definitions of parameterized problems FTP and para-NP differs only in a determinism/nondeterminism of
algorithm $\mathcal{A}$.
Without a formal proof, let us mention a known fact that $\FPT = \pNP \Leftrightarrow \Po = \NP$ \cite[p.~39]{Flum2006}\todo{original source}.
From the \pNP-hardness perspective, we can say that a problem is \pNPh if it is \NPh already for a constant value of the parameter
and also that \pNPh problems are not in \XP unless $\Po = \NP$ \cite[p.~41]{Flum2006}\todo{original source}. 


\subsection{W-hierarchy}

As stated by Cygan et al. \cite[p.~423]{Cygan2015},
there are thousands of natural problems which are \NPc and which are reducible to each other,
meaning that in this sense they are equally hard and thus we can say that
they occupy the same level of hardness.
However, we cannot say the same when talking about parameterized problems as it seems there are
different levels of hardness for such problems and, in this sense, even basic problems
seem to be differently hard as they occupy different hardness levels.
For this reason, W-hierarchy was introduced by Downey and Fellows \cite{Downey1999} as
an attempt to shed a light on these apparent differences in hardness of parameterized problems. 

The levels of W-hierarchy are marked as W[$t$] for $t \in \mathbb{N} \wedge t \geq 0$,
where each level represents its own complexity class.
The most important level for us will be W[1].
The $k$-\textsc{Clique}, \textsc{Independent Set} and \textsc{Partial Vertex Cover} problems
are examples of W[1]-complete problems.
We will not provide the exact definition and further description of W-hierarchy
because it is not important for the purposes of this work.
On the other hand, what is important is a fact that we interpret the \W-hardness as an evidence that
a problem is not fixed-parameter tractable.
This interpretation is based on a general assumption that $\FPT \neq \W$.
Further, this assumption is implied by the Exponential Time Hypothesis -- unproven conjecture,
formulated by Impagliazzo and Paturi \cite{Impagliazzo1999}, saying that there is no
algorithm subexponential in the number of variables for the $3$-\textsc{SAT} problem.
